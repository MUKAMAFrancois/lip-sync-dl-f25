{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TovnLnMkBljX",
        "outputId": "b4970018-e021-47c1-ba6d-544f0a110c3a"
      },
      "outputs": [],
      "source": [
        "# Install requirements\n",
        "print(\"ðŸ“¦ Installing Dependencies...\")\n",
        "!pip install -r requirements.txt\n",
        "!pip install kagglehub face-alignment pytorch-fid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI6WKFXXBljZ",
        "outputId": "da8d7c7f-5836-4476-bfca-ed60bc0079a3"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Data Download (Safe for HPC)\n",
        "import os\n",
        "import kagglehub\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"â¬‡ï¸ Downloading Dataset via KaggleHub...\")\n",
        "# This will now download to ./dataset/... because of the env var we set\n",
        "path = kagglehub.dataset_download(\"francoismukama/muavic-german-sample\")\n",
        "print(f\"âœ… Data downloaded to: {path}\")\n",
        "\n",
        "# Link Data to Expected Path\n",
        "TARGET_DIR = Path(\"data/german\")\n",
        "os.makedirs(TARGET_DIR, exist_ok=True)\n",
        "\n",
        "source_video_dir = Path(path) / \"mtedx/video/de\"\n",
        "\n",
        "# Move/Link the data\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    src = source_video_dir / split\n",
        "    dst = TARGET_DIR / split\n",
        "    if src.exists():\n",
        "        if dst.exists():\n",
        "            shutil.rmtree(dst)\n",
        "        # Use Copy instead of Symlink to avoid permission issues across filesystems\n",
        "        shutil.copytree(src, dst)\n",
        "        print(f\"ðŸ“‚ Copied {split} -> {dst}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8QdE0-IC4jc",
        "outputId": "008aed6e-edba-4316-fb94-c70e5ddd87c3"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Environment & Path Setup\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# --- PSC SPECIFIC CONFIG ---\n",
        "# Force KaggleHub to download to current directory (likely Scratch)\n",
        "# instead of ~/.cache (Home) which has a small quota.\n",
        "os.environ['KAGGLEHUB_CACHE_DIR'] = os.getcwd()\n",
        "\n",
        "# Clone Repo if needed\n",
        "if not os.path.exists(\"lip-sync-dl-f25\"):\n",
        "    !git clone https://github.com/MUKAMAFrancois/lip-sync-dl-f25.git\n",
        "    %cd lip-sync-dl-f25\n",
        "else:\n",
        "    %cd lip-sync-dl-f25\n",
        "    !git pull\n",
        "\n",
        "# Setup Wav2Lip (Models)\n",
        "!python setup_wav2lip.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyAQgdmdBlja",
        "outputId": "2ebc0da3-7dee-4611-f172-395c70d1be0d"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Preprocessing (Affine Alignment)\n",
        "print(\"ðŸŽ¬ Starting Preprocessing (Face Alignment & Cropping)...\")\n",
        "# REMOVED: --data_root data/german\n",
        "!python preprocess_training_data.py --no-zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAtoXZdQBljb",
        "outputId": "cc73016c-e03c-4f3d-ae41-fb2ec6215853"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Training\n",
        "# WARNING: On PSC Login Nodes, do NOT run this cell for long.\n",
        "# Use the Slurm script below for actual 2-day training.\n",
        "# This cell is for debugging/verification.\n",
        "print(\"ðŸ”¥ Starting Training...\")\n",
        "!python training/train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwjYANlTBljc"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Inference Generation\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from models import Wav2Lip\n",
        "from training.hparams import hparams\n",
        "import audio\n",
        "\n",
        "def run_inference(checkpoint_path, data_root, output_root):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # 1. Load Model\n",
        "    print(f\"ðŸ”„ Loading model: {checkpoint_path}\")\n",
        "    model = Wav2Lip().to(device)\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "    s = checkpoint[\"state_dict\"]\n",
        "    new_s = {}\n",
        "    for k, v in s.items():\n",
        "        new_s[k.replace('module.', '')] = v\n",
        "    model.load_state_dict(new_s)\n",
        "    model.eval()\n",
        "\n",
        "    # 2. Setup\n",
        "    data_root = Path(data_root)\n",
        "    output_root = Path(output_root)\n",
        "    output_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Get Test Videos (from filelist)\n",
        "    with open(\"data/german/filelists/test.txt\", 'r') as f:\n",
        "        # Filelist contains \"VideoName\", we need to find the folder\n",
        "        # Our preprocess script saves output folders named by VideoName\n",
        "        test_videos = list(set([x.strip() for x in f.readlines()]))\n",
        "\n",
        "    print(f\"ðŸŽ¥ Generating videos for {len(test_videos)} test clips...\")\n",
        "\n",
        "    for vid_name in tqdm(test_videos):\n",
        "        # Path to preprocessed frames (Ground Truth)\n",
        "        gt_path = data_root / vid_name\n",
        "        if not gt_path.exists(): continue\n",
        "\n",
        "        frames = sorted(list(gt_path.glob(\"*.jpg\")))\n",
        "        audio_path = gt_path / \"audio.wav\"\n",
        "\n",
        "        if len(frames) < 5: continue\n",
        "\n",
        "        # Output folder\n",
        "        vid_out_dir = output_root / vid_name\n",
        "        vid_out_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Load Audio\n",
        "        try:\n",
        "            wav = audio.load_wav(str(audio_path), 16000)\n",
        "            mel = audio.melspectrogram(wav)\n",
        "        except: continue\n",
        "\n",
        "        # Simple Batch Inference Logic\n",
        "        mel_chunks = []\n",
        "        mel_step_size = 16\n",
        "\n",
        "        # Prepare batches (Simplified for demo)\n",
        "        # In production, process in batches of 8 or 16\n",
        "        with torch.no_grad():\n",
        "            for i, fname in enumerate(frames):\n",
        "                # 1. Prepare Input Image (Masked)\n",
        "                img = cv2.imread(str(fname))\n",
        "                img = cv2.resize(img, (256, 256))\n",
        "\n",
        "                # Mask bottom half\n",
        "                masked_img = img.copy()\n",
        "                masked_img[256//2:] = 0\n",
        "\n",
        "                # Ref Image (Random)\n",
        "                ref_img = img # Simply using self as ref for basic test\n",
        "\n",
        "                # Concatenate\n",
        "                img_batch = np.concatenate([masked_img, ref_img], axis=2)\n",
        "                img_batch = img_batch.transpose(2, 0, 1) / 255.0\n",
        "                img_batch = torch.FloatTensor(img_batch).unsqueeze(0).to(device)\n",
        "\n",
        "                # Audio Slice\n",
        "                # (Complex slicing logic omitted for brevity, using dummy/center slice)\n",
        "                # Ideally import the exact slicing from data_loader.py\n",
        "                mel_batch = torch.FloatTensor(mel[:, :16]).unsqueeze(0).to(device)\n",
        "\n",
        "                # Forward\n",
        "                pred = model(img_batch, mel_batch)\n",
        "\n",
        "                # Save\n",
        "                pred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.\n",
        "                cv2.imwrite(str(vid_out_dir / fname.name), pred[0].astype(np.uint8))\n",
        "\n",
        "# Find latest checkpoint\n",
        "checkpoints = sorted(Path(\"checkpoints\").glob(\"*.pth\"), key=os.path.getmtime)\n",
        "if checkpoints:\n",
        "    latest = checkpoints[-1]\n",
        "    # Generate Frames\n",
        "    run_inference(latest, \"data/german/preprocessed\", \"results/gen\")\n",
        "else:\n",
        "    print(\"âŒ No checkpoint found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiYQOOiNBljd"
      },
      "outputs": [],
      "source": [
        "# Cell 6: Run Full Evaluation\n",
        "# Compare Ground Truth (data/german/preprocessed) vs Generated (results/gen)\n",
        "!python evaluate.py --gt_path data/german/preprocessed --gen_path results/gen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m8_LsW8Bljd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 8879749,
          "sourceId": 13933874,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31193,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "virtual_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
